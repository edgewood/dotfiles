import bs4
import logging
import random
import re
import requests
import subprocess

from time import sleep

from feed2exec.plugins.maildir import output as maildir_output

_skips = [
    # '([^/]+-)?' matches regional paper abbreviations, eg
    # /sh-sports/ for a sports story in the Smithfield Herald
    re.compile(r'.*/([^/]+-)?sports/'),
    re.compile(r'.*/lottery/'),
    re.compile(r'.*/([^/]+-)?obituaries/'),
    re.compile(r'.*/([^/]+-)?announcements-celebrations/'),
]


def filter(*args, feed=None, item=None, **kwargs):
    '''the newsobserver filter will drop feed items I don't want, and fetch the
    full text of the remaining ones.
    '''
   
    url = item.get('link')

    if not url or skip_url(url):
        logging.info('Skipping url "%s"', url)
        item['skip'] = True

def output(*args, feed=None, item=None, **kwargs):
    if feed.get('catchup'):
        return True

    sleep(random.randint(10, 30))

    item.pop('author_detail', '')       # don't want byline of item in email From:
    feed.pop('author_detail', '')       # don't want byline of feed in email From:
    url = item.get('link')
    text = extract_story(fetch_html(direct_url(url)))

    if text:
        item['description'] = "\n\n".join((url, text))

    return maildir_output(*args, feed=feed, item=item, **kwargs)

def skip_url(url):
    global _skips
    return any([pat.match(url) for pat in _skips])

def direct_url(url):
    """ Given an URL that may direct to a welcome page, generate the direct URL to that page"""
    return url.replace('#storylink=rss', '')

def fetch_html(url):
    cmd = [
        "curl",
        "--silent",
        "--header",
        "User-agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) "
        "Gecko/20100101 Firefox/52.0",
        url,
    ]
    # N&O resets connections using requests, so exec curl instead
    try:
        text = subprocess.check_output(cmd)
        logging.info('Fetched full text from "%s"', url)
        return text
    except subprocess.CalledProcessError as e:
        logging.info('Fetch of url "%s" returned %d', url, e.returncode)
        return None

def extract_story(html):
    if html is None:
        return None
    parsed_html = bs4.BeautifulSoup(html, features="lxml")
    story = []

    # the N&O frequently includes XML char references for Windows-1252 "smart quotes" in
    # its UTF-8 text. The corresponding UTF-8 characters aren't the same, so translate
    # them to what was intended.
    # Consider http://blog.luminoso.com/2012/08/20/fix-unicode-mistakes-with-python/ if
    # this gets too complicated.
    translate = str.maketrans({
        '\x85': '\u2026', # ellipsis
        '\x91': '\u2018', # left single quote
        '\x92': '\u2019', # right single quote
        '\x93': '\u201c', # left double quote
        '\x94': '\u201d', # right double quote
        '\x96': '\u2013', # en dash
        '\x97': '\u2014', # em dash
    })

    body = parsed_html.body.find('article', class_='story-body')

    if body is not None:
        header = body.header
        if header:      # the header has the byline, always include
            for part in header.find_all('p'):
                story.append(paragraph_text(part).translate(translate))

        for part in body.find_all(re.compile('^(span|p)$'), recursive=False):
            if part.name == "p" or "dateline" in part.attrs.get("class", []):
                story.append(paragraph_text(part).translate(translate))
    else:
        return None

    return '\n\n'.join([p for p in story if p != ''])

def paragraph_text(p):
    # return text of simple paragraphs
    if p.string:
        return p.string
    # delimit tweets
    if "class" in p.parent.attrs and "twitter" in " ".join(p.parent.attrs["class"]):
        return "[tweet]{}[/tweet]".format("".join(p.strings))
    # default to returning all strings in paragraph: I'd rather read around
    # garbage than silently miss parts
    return "".join([t.strip("\n\t") for t in p.strings])
